# -*- coding: utf-8 -*-
"""insurance_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1he4DN6cFxGJ2AJv3bwXxqgLC0fuYhPp-
"""
python -m pip install streamlit        
!pip install pandas
!pip install langchain
!pip install faiss-cpu
!pip install transformers
!pip install torch
!pip install openpyxl
!pip install sentence-transformers
!pip install langchain_community
import streamlit as st
import pandas as pd
import os
from langchain.llms import HuggingFacePipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.document_loaders.csv_loader import CSVLoader
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import gc

# Set page configuration
st.set_page_config(page_title="Insurance Knowledge Assistant", layout="wide")

# Function to load the small LLM model
@st.cache_resource
def load_llm():
    # Using a smaller model that can run on CPU or modest GPU
    model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # A small model example

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
    )

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=512,
        temperature=0.1,
        top_p=0.95,
        repetition_penalty=1.15
    )

    llm = HuggingFacePipeline(pipeline=pipe)
    return llm

# Function to load embeddings model
@st.cache_resource
def load_embeddings():
    # Using a small, efficient embedding model
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'}
    )
    return embeddings

# Function to process Excel data
@st.cache_data
def process_excel_data(uploaded_file):
    # Save the uploaded file temporarily
    with open("iciciprulife_faqs.csv", "wb") as f:
        f.write(uploaded_file.getbuffer())

    # Read csv file
    df = pd.read_csv("iciciprulife_faqs.csv", index=False)

    # Load documents
    loader = CSVLoader(file_path="iciciprulife_faqs.csv")
    documents = loader.load()

    # Split documents into chunks
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100
    )
    chunks = text_splitter.split_documents(documents)

    return chunks

# Function to create vector store
@st.cache_resource
def create_vectorstore(chunks, embeddings):
    vectorstore = FAISS.from_documents(chunks, embeddings)
    return vectorstore

# Initialize session state variables
if 'conversation' not in st.session_state:
    st.session_state.conversation = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'vector_store_created' not in st.session_state:
    st.session_state.vector_store_created = False

# App title and description
st.title("Insurance Knowledge Assistant")
st.subheader("Upload your insurance data or start chatting")

# Sidebar for data upload and system config
with st.sidebar:
    st.header("Upload Data")
    uploaded_file = st.file_uploader("Upload Insurance Excel Data", type=["xlsx", "xls"])

    if uploaded_file is not None and not st.session_state.vector_store_created:
        with st.spinner("Processing your insurance data..."):
            # Load embeddings
            embeddings = load_embeddings()

            # Process the Excel data
            chunks = process_excel_data(uploaded_file)

            # Create vector store
            vectorstore = create_vectorstore(chunks, embeddings)

            # Load LLM
            llm = load_llm()

            # Create memory
            memory = ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True
            )

            # Create conversation chain
            st.session_state.conversation = ConversationalRetrievalChain.from_llm(
                llm=llm,
                retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
                memory=memory,
                verbose=True
            )

            st.session_state.vector_store_created = True
            st.success("Data processed successfully! You can now start chatting.")

            # Display system information
            st.header("System Information")
            if torch.cuda.is_available():
              st.write(f"GPU: {torch.cuda.get_device_name(0)}")
              st.write(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            else:
              st.write("Running on CPU")

# Main chat interface
if st.session_state.vector_store_created:
    # Display chat history
    for i, message in enumerate(st.session_state.chat_history):
        if i % 2 == 0:
            st.write(f"üßë‚Äçüíº **You:** {message}")
        else:
            st.write(f"ü§ñ **Assistant:** {message}")

    # Input for new question
    user_question = st.text_input("Ask a question about the insurance data:")

    if user_question:
        if st.session_state.conversation is not None:
            st.spinner("Thinking...")

            # Get the conversation response
            response = st.session_state.conversation({"question": user_question})
            answer = response['answer']

            # Update chat history
            st.session_state.chat_history.append(user_question)
            st.session_state.chat_history.append(answer)

            # Force a rerun to display the updated chat
            st.experimental_rerun()
else:
    st.info("Please upload your insurance data first using the sidebar.")

# Clean up function
def cleanup():
    if os.path.exists("iciciprulife_faqs.csv"):
        os.remove("iciciprulife_faqs.csv")
    # Clear GPU memory if using GPU
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

# Register cleanup
st.cache_resource.clear()




